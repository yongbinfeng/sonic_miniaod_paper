%Previous work~\cite{Krupa:2020bwg,Duarte:2019fta,Wang:2020fjr}.

The SONIC approach~\cite{SONIC_origin, SONIC_cms} was introduced in the first notable use of the IaaS scheme in HEP, which employed FPGA coprocessors to accelerate ML algorithms~\cite{Duarte:2019fta}. In this application, the ResNet-50 convolutional neural network~\cite{ResNet50} was retrained as a top quark jet tagger and as a neutrino event classifier. These algorithms were then hosted on the Microsoft Brainwave service~\cite{Brainwave}. It was shown that IaaS enabled per-inference latency reductions by a factor of more than 30, even including overhead and data transfer time between client jobs at Fermilab in Illinois and the Microsoft data center in Virginia.

This work was then extended to study the use of GPUs as coprocessors for multiple LHC-specific applications in Ref.~\cite{Krupa:2020bwg}. There, SONIC was used to enable acceleration of three algorithms:
\begin{enumerate}
    \item A ResNet-50 based top quark jet tagger;
    \item Fast Calorimeter Learning (FACILE), which is a deep neural network for CMS hadron calorimeter (HCAL) energy regression;
    \item DeepCalo, which is a convolutional neural network for electron and photon energy regression in the ATLAS electromagnetic calorimeter~\cite{deepcalo}.
\end{enumerate}
These algorithms represent a wide range of algorithmic complexity and input data size, and all of them show significant speed improvements when running via SONIC. While these algorithms provided an interesting and useful testbed, currently they are not run by default in either the ATLAS or CMS data-processing workflows.

The initial FPGA-based demonstration was similarly extended to a detailed study of GPU-based acceleration in the neutrino experiment context in Ref.~\cite{Wang:2020fjr}. There, the track and particle shower hit identification components of the ProtoDUNE-SP reconstruction chain~\cite{DUNE:2020cqd} were accelerated by a factor of 17. This lead to a 2.7 times reduction in the total processing time per event compared to a CPU-only architecture. In this application, a single GPU could be used to service 68 CPU threads, which is an important demonstration of the flexibility to optimize the CPU-coprocessor ratios in the IaaS paradigm.

Heterogeneous computing frameworks using GPUs have appeared in multiple non-IaaS contexts in HEP recently as well. One of the first significant real-time applications was in the ALICE high-level trigger system~\cite{ALICE:2018phe}, where GPUs were used to accelerate online tracking. Similarly, a fully GPU-based implementation of the first level trigger has been planned for LHCb~\cite{Aaij:2019zbu}, which could be run on about 500 GPUs. For a further overview of GPU usage in real-time applications for HEP, please see Ref.~\cite{VomBruch:2020plx}.

In the CMS context, an algorithm that could accelerate tracking on GPUs with a parallelized cellular automaton approach was introduced in Ref.~\cite{Funke:2014dga}. Using a GPU-enabled heterogeneous architecture in the first layer of the CMS trigger for track triggering was then proposed in Ref.~\cite{Pantaleo:2016ery}, and a GPU-specific version of tracking called ``Patatrack'' has been developed which allows more complex tracking to be performed in triggering stages of data analysis~\cite{Bocci:2020pmi}. The authors of this algorithm proposed a mechanism for \CMSSW jobs to directly interact with local co-processor resources~\cite{Bocci:2020olh}. Similarly, the use of GPU and FPGA resources was investigated to take advantage of the inherent parallelizability of local reconstruction algorithms for the CMS Electromagnetic and Hadronic calorimeters~\cite{Massironi_2020}.%and the use of GPU and FPGA resources was investigated to take advantage of the inherent parallelizability of local reconstruction algorithms for the CMS Electromagnetic and Hadronic calorimeters~\cite{Massironi_2020}. More recently, a GPU-specific version of tracking called ``Patatrack'' has been developed which allows more complex tracking to be performed in triggering stages of data analysis~\cite{Bocci:2020pmi}, and the authors of this algorithm proposed a mechanism for \CMSSW jobs to directly interact with local co-processor resources~\cite{Bocci:2020olh}.


%In neutrino physics, GPUs have been used to accelerate particle reconstruction tasks for IceCube~\cite{IceCube_tracking}.