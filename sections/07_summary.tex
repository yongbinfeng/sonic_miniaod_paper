Within the next decade, the data taking rate at the LHC will increase dramatically, straining the expected computing resources of the experiments running on it. At the same time, more and more of the algorithms that will be run on these resources are being converted into either ML or domain algorithms that are easily accelerated with the use of GPUs and other coprocessors. Therefore, by pursuing heterogeneous architectures, it is possible to alleviate potential shortcomings of available CPU resources.

A promising scheme to integrate coprocessors into CMS computing workflows is that of Inference as-a-Service (IaaS). In IaaS, client code simply assembles the input data for an algorithm, sends that input to an inference server running either locally or remotely, and retrieves output from the server. The implementation of IaaS discussed throughout this paper is called SONIC. SONIC employs NVIDIA Triton Inference Servers to host models on coprocessors, as demonstrated here in studies on GPUs, CPUs, and IPUs. In general, using SONIC in CMSSW has many benefits relative to writing a custom mechanism for interacting with coprocessors: it factorizes ML frameworks out of CMSSW, allowing for more diverse algorithms; it simplifies client code, which only needs to send data to the server and retrieve output; it enables more efficient use of resources, as the coprocessor to CPU ratio can be tuned; client code does not need to be adjusted to use different types of coprocessors for inference acceleration; and it is the only way to use remote coprocessor resources.

In this paper, the use of SONIC in CMSSW is demonstrated in a sample MiniAOD workflow, where a jet tagging algorithm, a tau identification algorithm, and a missing transverse energy regression algorithm are ported to inference servers. These algorithms account for 10\% of per-event latency in a sample of Run 2 \ttbar events. After a demonstration of model-profiling, which is used to optimize server performance and determine the needed number of GPUs for a given number of client jobs, we showed that the expected 10\% decrease in per-event latency was achieved in a large scale test of SONIC-enabled MiniAOD production that used about 10\,000 CPU cores and 100 GPUs.

In addition to meeting performance expectations, we demonstrated that the per-event latency is not highly sensitive to physical client-to-server distance, and that running inference through Triton servers on local CPU resources does not increase latency relative to the standard approach of running inference directly on CPUs in the job thread. We were also able to perform a test of SONIC on GraphCore IPUs to demonstrate the flexibility of our approach.

The SONIC approach for IaaS represents a flexible approach to accelerate algorithms, which is increasingly valuable for LHC experiments. Using a realistic workflow, we have highlighted many of SONIC's benefits, which we believe make it a leading paradigm for the future of CMS computing.

%In this paper we have presented the studies of running inference as-a-Service via SONIC, with the CMS Mini-AOD production workflow as the motivating case. The benefits of running SONIC are discussed, and the studies of running different models with different ML backends are compared. We have observed the expected gains by offloading ML inferences to (remote) GPUs via SONIC, with negligible overhead for servers running at different sites. The whole framework can run also on different types of coprocessors such as IPUs, and scales up well for large-scale productions. Studies are also done for optimizing inference performances on local CPUs. 

%With more and more ML algorithms being developed and integrated into the production workflow, these studies are becoming more and more interesting and important to provide an approach for solving the computing challenge.